{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Index","text":""},{"location":"#labelformat-label-conversion-simplified","title":"Labelformat - Label Conversion, Simplified","text":"<p>Welcome to Labelformat\u2014an open-source tool designed to effortlessly convert between various popular computer vision label formats. Whether you're adapting labels from downloaded datasets or modifying outputs from labeling tools to suit your model's requirements, Labelformat streamlines the process, saving you time and reducing complexity.</p>"},{"location":"#why-choose-labelformat","title":"Why Choose Labelformat?","text":"<ul> <li>Comprehensive Format Support: Convert seamlessly between formats like COCO, YOLOv5-8, PascalVOC, KITTI, and more.</li> <li>Ease of Use: Intuitive CLI and Python API for flexible integration into your workflows.</li> <li>Efficiency: Memory-conscious processing ensures optimal performance, even with large datasets.</li> <li>Reliability: Thoroughly tested with round-trip tests to maintain label consistency.</li> </ul>"},{"location":"#get-started","title":"Get Started","text":"<ul> <li>Installation</li> <li>Quick Start Guide</li> <li>Explore Features</li> </ul>"},{"location":"#join-our-community","title":"Join Our Community","text":"<p>Labelformat is actively maintained by Lightly, a company dedicated to building efficient active learning pipelines. We welcome contributions and feedback\u2014check out our Contributing Guide to get involved!</p>"},{"location":"#quick-links","title":"\ud83d\udce6 Quick Links","text":"<ul> <li>GitHub Repository</li> <li>PyPI Package</li> <li>Documentation</li> </ul>"},{"location":"about-us/","title":"About Us","text":"<p>Labelformat is maintained by Lightly, a spin-off from ETH Zurich dedicated to building efficient active learning pipelines for machine learning models. Our mission is to empower data scientists and engineers with tools that streamline data processing and model training workflows.</p>"},{"location":"about-us/#our-mission","title":"Our Mission","text":"<p>At Lightly, we aim to simplify the complexities of active learning and data management, enabling teams to focus on developing cutting-edge machine learning models without getting bogged down by data preparation challenges.</p>"},{"location":"about-us/#what-we-offer","title":"What We Offer","text":"<ul> <li>Active Learning Pipelines: Intelligent data selection to enhance model performance with minimal data.</li> <li>Efficient Data Management: Tools and services that optimize data workflows for scalability and efficiency.</li> <li>Expert Support: Dedicated support to help you integrate our solutions seamlessly into your projects.</li> </ul>"},{"location":"about-us/#learn-more","title":"Learn More","text":"<ul> <li>Homepage</li> <li>Web-App</li> <li>Lightly Solution Documentation</li> <li>Contact Us</li> </ul>"},{"location":"about-us/#connect-with-us","title":"Connect with Us","text":"<p>Stay updated with the latest developments, tips, and tutorials by following us:</p> <ul> <li>GitHub</li> <li>Twitter</li> <li>LinkedIn</li> </ul> <p>Labelformat is part of Lightly's commitment to fostering an open-source ecosystem that benefits the global machine learning community. Join us in making data management and label conversion effortless!</p>"},{"location":"features/","title":"Features","text":"<p>Labelformat offers a robust set of features tailored to meet the diverse needs of computer vision engineers and data scientists.</p>"},{"location":"features/#key-features","title":"Key Features","text":"<ul> <li> <p>Wide Format Support:</p> <ul> <li>Object Detection </li> <li>Instance Segmentation</li> </ul> </li> <li> <p>User-Friendly CLI and Python API:</p> <ul> <li>CLI: Simple commands to convert formats with customizable options.</li> <li>Python API: Integrate label conversion seamlessly into your Python workflows.</li> </ul> </li> <li> <p>Performance Optimizations:</p> <ul> <li>Memory Conscious: Processes datasets file-by-file to minimize memory usage.</li> <li>Minimal Dependencies: Targets Python 3.7 or higher, ensuring broad compatibility.</li> </ul> </li> <li> <p>Reliability and Testing:</p> <ul> <li>Typed Codebase: Ensures type safety and easier maintenance.</li> <li>Round-Trip Tests: Guarantees label consistency across conversions.</li> </ul> </li> <li> <p>Open-Source and Community-Driven:</p> <ul> <li>MIT License: Free to use and modify.</li> <li>Active Contributions: Regular updates and community support.</li> </ul> </li> </ul>"},{"location":"features/#supported-tasks-and-formats","title":"Supported Tasks and Formats","text":""},{"location":"features/#object-detection","title":"Object Detection","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox (input only)</li> <li>Lightly</li> <li>PascalVOC</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> </ul>"},{"location":"features/#why-labelformat","title":"Why Labelformat?","text":"<p>Labelformat addresses the common challenges faced when dealing with diverse label formats:</p> <ul> <li>Consistency: Ensures uniformity across different formats, crucial for model training.</li> <li>Efficiency: Reduces the time spent on manual label format conversions.</li> <li>Scalability: Handles large datasets with minimal memory footprint.</li> <li>Flexibility: Supports a growing list of formats and tasks, adapting to evolving project needs.</li> </ul> <p>Explore our Quick Start Guide to begin leveraging Labelformat's powerful features today!</p>"},{"location":"installation/","title":"Installation","text":"<p>Installing Labelformat is straightforward. Follow the steps below to set up Labelformat in your development environment.</p>"},{"location":"installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.7 or higher: Ensure you have Python installed. You can download it from the official website.</li> <li>pip: Python's package installer. It typically comes with Python installations.</li> </ul>"},{"location":"installation/#installation-via-pypi","title":"Installation via PyPI","text":"<p>You can install Labelformat directly from PyPI using pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"installation/#installation-from-source","title":"Installation from Source","text":"<p>If you prefer to install Labelformat from the source code, follow these steps:</p> <ol> <li>Clone the Repository:    <pre><code>git clone https://github.com/lightly-ai/labelformat.git\ncd labelformat\n</code></pre></li> <li>Install Dependencies:    Labelformat uses Poetry for dependency management. Ensure you have Poetry installed:    <pre><code>pip install poetry\n</code></pre></li> <li>Set Up the Development Environment:    <pre><code>poetry install\n</code></pre></li> </ol>"},{"location":"installation/#updating-labelformat","title":"Updating Labelformat","text":"<p>To update Labelformat to the latest version, run: <pre><code>pip install --upgrade labelformat\n</code></pre></p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get up and running with Labelformat in minutes! This Quick Start Guide provides simple, copy-paste examples to help you convert label formats effortlessly.</p>"},{"location":"quick-start/#scenario-1-convert-coco-to-yolov8-using-cli","title":"Scenario 1: Convert COCO to YOLOv8 Using CLI","text":""},{"location":"quick-start/#step-1-prepare-your-files","title":"Step 1: Prepare Your Files","text":"<p>Ensure you have the following structure: <pre><code>project/\n\u251c\u2500\u2500 coco-labels/\n\u2502   \u2514\u2500\u2500 train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n</code></pre></p>"},{"location":"quick-start/#step-2-run-the-conversion-command","title":"Step 2: Run the Conversion Command","text":"<p>Open your terminal, navigate to your project directory, and execute:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/train.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"quick-start/#step-3-verify-the-output","title":"Step 3: Verify the Output","text":"<p>Your project structure should now include:</p> <pre><code>project/\n\u251c\u2500\u2500 yolo-labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n</code></pre>"},{"location":"quick-start/#scenario-2-convert-yolov8-to-coco-using-python-api","title":"Scenario 2: Convert YOLOv8 to COCO Using Python API","text":""},{"location":"quick-start/#step-1-install-labelformat","title":"Step 1: Install Labelformat","text":"<p>If you haven't installed Labelformat yet, do so via pip: <pre><code>pip install labelformat\n</code></pre></p>"},{"location":"quick-start/#step-2-write-the-conversion-script","title":"Step 2: Write the Conversion Script","text":"<p>Create a Python script, <code>convert_yolo_to_coco.py</code>, with the following content:</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionOutput, YOLOv8ObjectDetectionInput\n\n# Load YOLOv8 labels\nyolo_input = YOLOv8ObjectDetectionInput(\n    input_file=Path(\"yolo-labels/data.yaml\"),\n    input_split=\"train\"\n)\n\n# Convert to COCO format and save\ncoco_output = COCOObjectDetectionOutput(\n    output_file=Path(\"coco-from-yolo/converted_coco.json\")\n)\ncoco_output.save(label_input=yolo_input)\n\nprint(\"Conversion from YOLOv8 to COCO completed successfully!\")\n</code></pre>"},{"location":"quick-start/#step-3-execute-the-script","title":"Step 3: Execute the Script","text":"<p>Run the script:</p> <pre><code>python convert_yolo_to_coco.py\n</code></pre>"},{"location":"quick-start/#step-4-check-the-coco-output","title":"Step 4: Check the COCO Output","text":"<p>Your project should now have:</p> <pre><code>project/\n\u251c\u2500\u2500 coco-from-yolo/\n\u2502   \u2514\u2500\u2500 converted_coco.json\n</code></pre>"},{"location":"quick-start/#scenario-3-convert-labelbox-export-to-lightly-format","title":"Scenario 3: Convert Labelbox Export to Lightly Format","text":""},{"location":"quick-start/#step-1-export-labels-from-labelbox","title":"Step 1: Export Labels from Labelbox","text":"<p>Ensure you have the Labelbox export file, e.g., <code>labelbox-export.ndjson</code>.</p>"},{"location":"quick-start/#step-2-run-the-conversion-command_1","title":"Step 2: Run the Conversion Command","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format lightly \\\n    --output-folder lightly-labels/annotation-task\n</code></pre>"},{"location":"quick-start/#step-3-verify-the-lightly-output","title":"Step 3: Verify the Lightly Output","text":"<p>Your project structure should include:</p> <pre><code>project/\n\u251c\u2500\u2500 lightly-labels/\n\u2502   \u251c\u2500\u2500 annotation-task/\n\u2502   \u2502   \u251c\u2500\u2500 schema.json\n\u2502   \u2502   \u251c\u2500\u2500 image1.json\n\u2502   \u2502   \u2514\u2500\u2500 image2.json\n</code></pre>"},{"location":"usage/","title":"usage.md","text":""},{"location":"usage/#detailed-usage-guide","title":"Detailed Usage Guide","text":"<p>Labelformat offers both a Command-Line Interface (CLI) and a Python API to cater to different workflows. This guide provides in-depth instructions on how to use both interfaces effectively.</p>"},{"location":"usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>CLI Usage</li> <li>Basic Conversion Command</li> <li>Advanced CLI Options</li> <li>Python API Usage</li> <li>Basic Conversion</li> <li>Customizing Conversion</li> <li>Common Tasks</li> <li>Handling Category Names</li> <li>Managing Image Paths</li> </ul>"},{"location":"usage/#cli-usage","title":"CLI Usage","text":"<p>Labelformat's CLI provides a straightforward way to convert label formats directly from the terminal.</p>"},{"location":"usage/#basic-conversion-command","title":"Basic Conversion Command","text":"<p>Example: Convert Object Detection labels from COCO to YOLOv8.</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file path/to/coco/train.json \\\n    --output-format yolov8 \\\n    --output-file path/to/yolo/data.yaml \\\n    --output-split train\n</code></pre> <p>Parameters:</p> <ul> <li><code>--task</code>: Specify the task type (<code>object-detection</code> or <code>instance-segmentation</code>).</li> <li><code>--input-format</code>: The format of the input labels (e.g., <code>coco</code>).</li> <li><code>--input-file</code> or <code>--input-folder</code>: Path to the input label file or folder.</li> <li><code>--output-format</code>: The desired output label format (e.g., <code>yolov8</code>).</li> <li><code>--output-file</code> or <code>--output-folder</code>: Path to save the converted labels.</li> <li><code>--output-split</code>: Define the data split (<code>train</code>, <code>val</code>, <code>test</code>).</li> </ul>"},{"location":"usage/#advanced-cli-options","title":"Advanced CLI Options","text":"<p>Listing Supported Formats:</p> <p>To see all supported input and output formats for a specific task:</p> <pre><code>labelformat convert --task object-detection --help\n</code></pre> <p>Specifying Category Names:</p> <p>Some formats require explicit category names. Use the <code>--category-names</code> argument:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-output/train.json \\\n    --output-split train\n</code></pre> <p>Handling Missing Images:</p> <p>When converting formats that require image files (e.g., YOLO to COCO), ensure your image paths are correctly specified. Use <code>--images-rel-path</code> to define the relative path to images:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels/labels \\\n    --images-rel-path ../images \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"usage/#python-api-usage","title":"Python API Usage","text":"<p>For more flexible integrations, Labelformat provides a Python API.</p>"},{"location":"usage/#basic-conversion","title":"Basic Conversion","text":"<p>Example: Convert COCO to YOLOv8.</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Initialize input and output classes\ncoco_input = COCOObjectDetectionInput(input_file=Path(\"coco-labels/train.json\"))\nyolo_output = YOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_output.save(label_input=coco_input)\n\nprint(\"Conversion from COCO to YOLOv8 completed successfully!\")\n</code></pre>"},{"location":"usage/#customizing-conversion","title":"Customizing Conversion","text":"<p>Example: Adding Custom Fields or Handling Special Cases.</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOInstanceSegmentationInput, YOLOv8InstanceSegmentationOutput\n\n# Initialize input for instance segmentation\ncoco_inst_input = COCOInstanceSegmentationInput(input_file=Path(\"coco-instance/train.json\"))\n\n# Initialize YOLOv8 instance segmentation output\nyolo_inst_output = YOLOv8InstanceSegmentationOutput(\n    output_file=Path(\"yolo-instance-labels/data.yaml\"),\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_inst_output.save(label_input=coco_inst_input)\n\nprint(\"Instance segmentation conversion completed successfully!\")\n</code></pre>"},{"location":"usage/#common-tasks","title":"Common Tasks","text":""},{"location":"usage/#handling-category-names","title":"Handling Category Names","text":"<p>Some label formats require you to specify category names explicitly. Ensure that category names are consistent across your dataset.</p> <p>Example:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-export.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-output/train.json \\\n    --output-split train\n</code></pre>"},{"location":"usage/#managing-image-paths","title":"Managing Image Paths","text":"<p>When converting formats that reference image files, accurately specify the relative paths to avoid missing files.</p> <p>Example:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels/labels \\\n    --images-rel-path ../images \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"usage/#tips-and-best-practices","title":"Tips and Best Practices","text":"<ul> <li>Backup Your Data: Always keep a backup of your original labels before performing conversions.</li> <li>Validate Output: After conversion, verify the output labels to ensure accuracy.</li> <li>Consistent Naming: Maintain consistent naming conventions for categories and files across different formats.</li> <li>Leverage Round-Trip Tests: Use Labelformat's testing capabilities to ensure label consistency when converting back and forth between formats.</li> </ul> <p>For more detailed examples and advanced usage scenarios, explore our Tutorials section.</p>"},{"location":"blog/","title":"Blog","text":""},{"location":"formats/","title":"Supported Formats","text":""},{"location":"formats/#object-detection","title":"Object Detection","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox</li> <li>Lightly</li> <li>PascalVOC</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> </ul>"},{"location":"formats/object-detection/","title":"Object Detection Overview","text":"<p>Object detection is a computer vision task that involves identifying and locating objects within images using rectangular bounding boxes. Each detection includes:</p> <ul> <li>A category label (e.g., \"car\", \"person\", \"dog\")</li> <li>A bounding box defining the object's location and size</li> <li>Optional confidence score indicating detection certainty</li> </ul> <p>Labelformat supports converting between major object detection annotation formats like COCO, YOLO, and Pascal VOC while preserving the essential bounding box coordinates and category information.</p>"},{"location":"formats/object-detection/#supported-formats","title":"Supported Formats","text":"<ul> <li>COCO</li> <li>KITTI</li> <li>Labelbox</li> <li>Lightly</li> <li>PascalVOC</li> <li>YOLOv5</li> <li>YOLOv6</li> <li>YOLOv7</li> <li>YOLOv8</li> <li>YOLOv9</li> <li>YOLOv10</li> <li>YOLOv11</li> </ul>"},{"location":"formats/object-detection/coco/","title":"COCO Object Detection Format","text":""},{"location":"formats/object-detection/coco/#overview","title":"Overview","text":"<p>COCO (Common Objects in Context) is a large-scale object detection dataset format developed by Microsoft. The format has become one of the most widely adopted standards for object detection tasks. You can find the complete format specification in the official COCO documentation.</p>"},{"location":"formats/object-detection/coco/#specification-of-coco-detection-format","title":"Specification of COCO Detection Format","text":"<p>COCO uses a single JSON file containing all annotations. The format consists of three main components:</p> <ul> <li>Images: Defines metadata for each image in the dataset.</li> <li>Categories: Defines the object classes.</li> <li>Annotations: Defines object instances.</li> </ul>"},{"location":"formats/object-detection/coco/#images","title":"Images","text":"<p>Defines metadata for each image in the dataset: <pre><code>{\n  \"id\": 0,                    // Unique image ID\n  \"file_name\": \"image1.jpg\",  // Image filename\n  \"width\": 640,              // Image width in pixels\n  \"height\": 416              // Image height in pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#categories","title":"Categories","text":"<p>Defines the object classes: <pre><code>{\n  \"id\": 0,                    // Unique category ID\n  \"name\": \"cat\"              // Category name\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#annotations","title":"Annotations","text":"<p>Defines object instances: <pre><code>{\n  \"image_id\": 0,              // Reference to image\n  \"category_id\": 2,           // Reference to category\n  \"bbox\": [540.0, 295.0, 23.0, 18.0]  // [x, y, width, height] in absolute pixels\n}\n</code></pre></p>"},{"location":"formats/object-detection/coco/#directory-structure-of-coco-dataset","title":"Directory Structure of COCO Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/                   # Image files\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations.json         # Single JSON file containing all annotations\n</code></pre>"},{"location":"formats/object-detection/coco/#converting-with-labelformat","title":"Converting with Labelformat","text":""},{"location":"formats/object-detection/coco/#command-line-interface","title":"Command Line Interface","text":"<p>Convert COCO format to YOLOv8: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/annotations.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre></p> <p>Convert YOLOv8 format to COCO: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file yolo-labels/data.yaml \\\n    --input-split train \\\n    --output-format coco \\\n    --output-file coco-labels/annotations.json\n</code></pre></p>"},{"location":"formats/object-detection/coco/#python-api","title":"Python API","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Load COCO format\nlabel_input = COCOObjectDetectionInput(\n    input_file=Path(\"coco-labels/annotations.json\")\n)\n\n# Convert to YOLOv8 format\nYOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\",\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/coco/#example","title":"Example","text":"<p>Complete annotations.json example: <pre><code>{\n  \"images\": [\n    {\n      \"id\": 0,\n      \"file_name\": \"image1.jpg\",\n      \"width\": 640,\n      \"height\": 416\n    }\n  ],\n  \"categories\": [\n    {\n      \"id\": 0,\n      \"name\": \"cat\"\n    }\n  ],\n  \"annotations\": [\n    {\n      \"image_id\": 0,\n      \"category_id\": 0,\n      \"bbox\": [540.0, 295.0, 23.0, 18.0]\n    }\n  ]\n}\n</code></pre></p>"},{"location":"formats/object-detection/kitti/","title":"KITTI Object Detection Format","text":""},{"location":"formats/object-detection/kitti/#overview","title":"Overview","text":"<p>The KITTI format was developed as part of the KITTI Vision Benchmark Suite, focusing on autonomous driving scenarios. This format is particularly well-suited for 3D object detection and tracking tasks. The complete format specification can be found in the KITTI development kit documentation.</p>"},{"location":"formats/object-detection/kitti/#specification-of-kitti-detection-format","title":"Specification of KITTI Detection Format","text":"<p>Each object is represented by 15 space-separated values:</p> <pre><code>#Values    Name      Description\n----------------------------------------------------------------------------\n   1    type         Object type (Car, Van, Truck, etc.)\n   1    truncated    Float 0-1 (truncated ratio)\n   1    occluded     Integer (0=visible, 1=partly occluded, 2=fully occluded)\n   1    alpha        Observation angle (-pi..pi)\n   4    bbox         2D bounding box (x1,y1,x2,y2) in pixels\n   3    dimensions   3D dimensions (height, width, length) in meters\n   3    location     3D location (x,y,z) in camera coordinates\n   1    rotation_y   Rotation around Y-axis in camera coordinates\n</code></pre>"},{"location":"formats/object-detection/kitti/#directory-structure-of-kitti-dataset","title":"Directory Structure of KITTI Dataset","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 000000.png\n\u2502   \u2514\u2500\u2500 000001.png\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 000000.txt\n    \u2514\u2500\u2500 000001.txt\n</code></pre>"},{"location":"formats/object-detection/kitti/#label-format","title":"Label Format","text":"<pre><code># Example: 000000.txt\nCar -1 -1 -10 614 181 727 284 -1 -1 -1 -1000 -1000 -1000 -10\nPedestrian -1 -1 -10 123 456 789 012 -1 -1 -1 -1000 -1000 -1000 -10\n</code></pre> <p>Note: The filename of each label file must match its corresponding image file, with .txt extension.</p>"},{"location":"formats/object-detection/kitti/#annotation-format-conversion","title":"Annotation Format Conversion","text":""},{"location":"formats/object-detection/kitti/#using-cli","title":"Using CLI","text":"<p>Convert from YOLOv8 to KITTI format: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format yolov8 \\\n    --input-file yolo-labels/data.yaml \\\n    --input-split train \\\n    --output-format kitti \\\n    --output-folder kitti-labels\n</code></pre></p> <p>Convert from KITTI to YOLOv8 format: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format kitti \\\n    --input-folder kitti-labels \\\n    --category-names car,pedestrian,cyclist \\\n    --images-rel-path ../images \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre></p>"},{"location":"formats/object-detection/kitti/#using-python","title":"Using Python","text":"<pre><code>from pathlib import Path\nfrom labelformat.formats import KittiObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Load KITTI labels\nlabel_input = KittiObjectDetectionInput(\n    input_folder=Path(\"kitti-labels\"),\n    category_names=\"car,pedestrian,cyclist\",\n    images_rel_path=\"../images\"\n)\n\n# Convert to YOLOv8 and save\nYOLOv8ObjectDetectionOutput(\n    output_file=Path(\"yolo-labels/data.yaml\"),\n    output_split=\"train\"\n).save(label_input=label_input)\n</code></pre>"},{"location":"formats/object-detection/kitti/#notes","title":"Notes","text":"<ul> <li>KITTI format uses absolute pixel coordinates (x1,y1,x2,y2) for bounding boxes</li> <li>Some fields like truncated, occluded, dimensions etc. are optional and can be set to -1 if unknown</li> <li>The category name (type) should match one of the predefined categories when converting </li> </ul>"},{"location":"formats/object-detection/labelbox/","title":"Labelbox Object Detection Format","text":""},{"location":"formats/object-detection/labelbox/#overview","title":"Overview","text":"<p>Labelbox uses NDJSON (Newline Delimited JSON) format for label exports, where each line represents a single image and its annotations. The format supports object detection through bounding boxes. While Labelformat currently supports Labelbox as an input-only format, you can find the complete format specification in the Labelbox documentation.</p>"},{"location":"formats/object-detection/labelbox/#specification-of-labelbox-detection-format","title":"Specification of Labelbox Detection Format","text":"<pre><code>dataset/\n\u2514\u2500\u2500 export-result.ndjson\n</code></pre> <p>Each line in the NDJSON file contains a complete JSON object with three main sections:</p> <ul> <li><code>data_row</code>: Contains image metadata (id, filename, external references)</li> <li><code>media_attributes</code>: Image dimensions</li> <li><code>projects</code>: Contains the actual annotations</li> </ul>"},{"location":"formats/object-detection/labelbox/#label-format","title":"Label Format","text":"<p>Each annotation line follows this structure: <pre><code>{\n  \"data_row\": {\n    \"id\": \"data_row_id\",\n    \"global_key\": \"image1.jpg\",\n    \"external_id\": \"image1.jpg\"\n  },\n  \"media_attributes\": {\n    \"width\": 640,\n    \"height\": 480\n  },\n  \"projects\": {\n    \"project_id\": {\n      \"labels\": [{\n        \"annotations\": {\n          \"objects\": [{\n            \"name\": \"cat\",\n            \"annotation_kind\": \"ImageBoundingBox\",\n            \"bounding_box\": {\n              \"top\": 100,\n              \"left\": 200,\n              \"width\": 50,\n              \"height\": 30\n            }\n          }]\n        }\n      }]\n    }\n  }\n}\n</code></pre></p>"},{"location":"formats/object-detection/labelbox/#converting-from-labelbox-format","title":"Converting from Labelbox Format","text":"<p>Labelbox format can be converted to other formats using labelformat. Here's an example converting to YOLOv8:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format labelbox \\\n    --input-file labelbox-labels/export-result.ndjson \\\n    --category-names cat,dog,fish \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"formats/object-detection/labelbox/#important-parameters","title":"Important Parameters","text":"<ul> <li><code>--category-names</code>: Required list of category names (comma-separated)</li> <li><code>--filename-key</code>: Which key to use as filename (options: global_key, external_id, id; default: global_key)</li> </ul>"},{"location":"formats/object-detection/labelbox/#format-details","title":"Format Details","text":""},{"location":"formats/object-detection/labelbox/#bounding-box-format","title":"Bounding Box Format","text":"<ul> <li>Uses absolute pixel coordinates</li> <li>Format: <code>{top, left, width, height}</code></li> <li>Origin: Top-left corner of the image</li> </ul>"},{"location":"formats/object-detection/labelbox/#limitations","title":"Limitations","text":"<ul> <li>Currently supports single project exports only</li> <li>Video annotations are not supported</li> <li>Only <code>ImageBoundingBox</code> annotation types are processed</li> </ul>"},{"location":"formats/object-detection/labelbox/#example","title":"Example","text":"<pre><code>{\"data_row\":{\"id\":\"ckz...\",\"global_key\":\"image1.jpg\",\"external_id\":\"img_1\"},\"media_attributes\":{\"width\":640,\"height\":480},\"projects\":{\"proj_123\":{\"labels\":[{\"annotations\":{\"objects\":[{\"name\":\"cat\",\"annotation_kind\":\"ImageBoundingBox\",\"bounding_box\":{\"top\":100,\"left\":200,\"width\":50,\"height\":30}}]}}]}}}\n{\"data_row\":{\"id\":\"ckz...\",\"global_key\":\"image2.jpg\",\"external_id\":\"img_2\"},\"media_attributes\":{\"width\":640,\"height\":480},\"projects\":{\"proj_123\":{\"labels\":[{\"annotations\":{\"objects\":[{\"name\":\"dog\",\"annotation_kind\":\"ImageBoundingBox\",\"bounding_box\":{\"top\":150,\"left\":300,\"width\":60,\"height\":40}}]}}]}}}\n</code></pre> <p>Note: This format is supported for input only in labelformat. </p>"},{"location":"formats/object-detection/lightly/","title":"Lightly Object Detection Format","text":""},{"location":"formats/object-detection/lightly/#overview","title":"Overview","text":"<p>The Lightly format is designed for efficient handling of object detection predictions in machine learning workflows. It provides a straightforward structure that's easy to parse and generate. For detailed information about the prediction format, refer to the Lightly AI documentation.</p>"},{"location":"formats/object-detection/lightly/#specification-of-lightly-detection-format","title":"Specification of Lightly Detection Format","text":"<p>The format uses a JSON file per image containing: - <code>file_name</code>: Name of the image file - <code>predictions</code>: List of object detections   - <code>category_id</code>: Integer ID of the object category   - <code>bbox</code>: List of [x, y, width, height] in absolute pixel coordinates   - <code>score</code>: Optional confidence score (0-1)</p>"},{"location":"formats/object-detection/lightly/#file-structure","title":"File Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 predictions/\n    \u251c\u2500\u2500 image1.json\n    \u2514\u2500\u2500 image2.json\n</code></pre>"},{"location":"formats/object-detection/lightly/#example","title":"Example","text":"<pre><code>{\n  \"file_name\": \"image1.jpg\",\n  \"predictions\": [\n    {\n      \"category_id\": 0,\n      \"bbox\": [100, 200, 50, 30],\n      \"score\": 0.95\n    },\n    {\n      \"category_id\": 1,\n      \"bbox\": [300, 400, 80, 60],\n      \"score\": 0.87\n    }\n  ]\n}\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/","title":"PascalVOC Object Detection Format","text":""},{"location":"formats/object-detection/pascalvoc/#overview","title":"Overview","text":"<p>PascalVOC (Visual Object Classes) is a widely used format for object detection tasks, introduced in the seminal paper \"The PASCAL Visual Object Classes (VOC) Challenge\" by Everingham et al. It stores annotations in XML files, with one XML file per image containing bounding box coordinates and class labels. The complete format specification is available in the PascalVOC development kit.</p>"},{"location":"formats/object-detection/pascalvoc/#specification","title":"Specification","text":"<p>Each XML annotation file contains: - Image metadata (filename, size, etc.) - List of objects, each with:   - Class name (string, allows spaces, e.g., \"traffic light\" or \"stop sign\")   - Bounding box coordinates as integer pixel values:     - xmin: left-most pixel coordinate     - ymin: top-most pixel coordinate     - xmax: right-most pixel coordinate     - ymax: bottom-most pixel coordinate   - Optional attributes (difficult, truncated, occluded)</p>"},{"location":"formats/object-detection/pascalvoc/#directory-structure","title":"Directory Structure","text":"<pre><code>dataset/\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n\u2514\u2500\u2500 annotations/\n    \u251c\u2500\u2500 image1.xml\n    \u2514\u2500\u2500 image2.xml\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#example-annotation","title":"Example Annotation","text":"<pre><code>&lt;annotation&gt;\n    &lt;folder&gt;images&lt;/folder&gt;\n    &lt;filename&gt;image1.jpg&lt;/filename&gt;\n    &lt;size&gt;\n        &lt;width&gt;640&lt;/width&gt;\n        &lt;height&gt;480&lt;/height&gt;\n        &lt;depth&gt;3&lt;/depth&gt;\n    &lt;/size&gt;\n    &lt;object&gt;\n        &lt;name&gt;cat&lt;/name&gt;\n        &lt;pose&gt;Unspecified&lt;/pose&gt;\n        &lt;truncated&gt;0&lt;/truncated&gt;\n        &lt;difficult&gt;0&lt;/difficult&gt;\n        &lt;bndbox&gt;\n            &lt;xmin&gt;100&lt;/xmin&gt;\n            &lt;ymin&gt;200&lt;/ymin&gt;\n            &lt;xmax&gt;300&lt;/xmax&gt;\n            &lt;ymax&gt;400&lt;/ymax&gt;\n        &lt;/bndbox&gt;\n    &lt;/object&gt;\n&lt;/annotation&gt;\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#format-details","title":"Format Details","text":"<ul> <li>Coordinates are in absolute pixel values (not normalized)</li> <li>Bounding boxes use XYXY format (xmin, ymin, xmax, ymax)</li> <li>Each object can have optional attributes:</li> <li><code>difficult</code>: Indicates hard to recognize objects</li> <li><code>truncated</code>: Indicates objects partially outside the image</li> <li><code>occluded</code>: Indicates partially obscured objects</li> </ul>"},{"location":"formats/object-detection/pascalvoc/#converting-with-labelformat","title":"Converting with Labelformat","text":""},{"location":"formats/object-detection/pascalvoc/#coco-to-pascalvoc","title":"COCO to PascalVOC","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/annotations.json \\\n    --output-format pascalvoc \\\n    --output-folder pascalvoc-labels\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#pascalvoc-to-coco","title":"PascalVOC to COCO","text":"<pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format pascalvoc \\\n    --input-folder pascalvoc-labels \\\n    --category-names cat,dog,fish \\\n    --output-format coco \\\n    --output-file coco-labels/annotations.json\n</code></pre>"},{"location":"formats/object-detection/pascalvoc/#required-arguments","title":"Required Arguments","text":"<ul> <li>For input:</li> <li><code>--input-folder</code>: Directory containing PascalVOC XML files</li> <li><code>--category-names</code>: Comma-separated list of category names (e.g., 'dog,cat')</li> <li>For output:</li> <li><code>--output-folder</code>: Directory to save generated XML files</li> </ul>"},{"location":"formats/object-detection/pascalvoc/#references","title":"References","text":"<ul> <li>Original PascalVOC Dataset</li> <li>Format Documentation </li> </ul>"},{"location":"formats/object-detection/yolov10/","title":"YOLOv10 Object Detection Format","text":""},{"location":"formats/object-detection/yolov10/#overview","title":"Overview","text":"<p>YOLOv10 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv10: Real-Time End-to-End Object Detection. Building upon the foundations of YOLOv5 through YOLOv9, YOLOv10 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv10 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv10's enhanced performance.</p>"},{"location":"formats/object-detection/yolov10/#key-yolov10-model-features","title":"Key YOLOv10 Model Features","text":"<p>YOLOv10 maintains full compatibility with the label format used in YOLOv5-v9, while introducing several key innovations:</p> <ul> <li>Adaptive Feature Fusion: Novel architecture that dynamically adjusts feature importance based on input complexity, leading to improved detection accuracy across varying scales.</li> <li>Resource-Aware Inference: Intelligent compute allocation that reduces GPU memory usage by up to 30% without sacrificing detection performance.</li> <li>Enhanced Small Object Detection: Specialized attention mechanisms that boost detection accuracy for small objects while maintaining real-time inference speeds.</li> <li>Automated Architecture Search: Built-in neural architecture search capabilities that optimize model configurations for specific hardware platforms.</li> </ul> <p>These improvements are handled internally by the model architecture and training pipeline, requiring no changes to the basic annotation format described below.</p>"},{"location":"formats/object-detection/yolov10/#specification-of-yolov10-detection-format","title":"Specification of YOLOv10 Detection Format","text":"<p>The YOLOv10 detection format remains consistent with previous versions (v5-v9), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov10/#directory-structure-of-yolov10-dataset","title":"Directory Structure of YOLOv10 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov10/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov10/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov10/#benefits-of-yolov10-format","title":"Benefits of YOLOv10 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov10/#example-of-yolov10-format","title":"Example of YOLOv10 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov10/#normalizing-bounding-box-coordinates-for-yolov10","title":"Normalizing Bounding Box Coordinates for YOLOv10","text":"<p>To convert pixel values to normalized values required by YOLOv10:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov10/#converting-annotations-to-yolov10-format-with-labelformat","title":"Converting Annotations to YOLOv10 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv10 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov10/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov10/#conversion-example-coco-to-yolov10","title":"Conversion Example: COCO to YOLOv10","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv10. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv10: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov10 \\\n    --output-folder dataset/yolov10_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov10_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov10/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov11/","title":"YOLOv11 Object Detection Format","text":""},{"location":"formats/object-detection/yolov11/#overview","title":"Overview","text":"<p>YOLOv11 is the latest iteration in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLOv5 through YOLOv10, YOLOv11 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv11 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv11's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv11 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv11 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov11</p>"},{"location":"formats/object-detection/yolov11/#key-yolov11-model-features","title":"Key YOLOv11 Model Features","text":"<p>YOLOv11 maintains full compatibility with the label format used in YOLOv5-v10, while introducing several groundbreaking capabilities:</p> <ul> <li>Enhanced Instance Segmentation: YOLOv11 adds support for polygon-based instance segmentation annotations (documented separately in the instance-segmentation format guide).</li> <li>Multi-Task Learning: The model can simultaneously handle object detection, instance segmentation, and pose estimation tasks using the same base format with task-specific extensions.</li> <li>Advanced Data Augmentation: While the base format remains the same, YOLOv11 introduces new augmentation techniques that can be applied during training without modifying the original annotations.</li> </ul> <p>These advanced features are handled internally by the model architecture and training pipeline, requiring no changes to the basic annotation format described below.</p>"},{"location":"formats/object-detection/yolov11/#specification-of-yolov11-detection-format","title":"Specification of YOLOv11 Detection Format","text":"<p>The YOLOv11 detection format remains consistent with previous versions (v5-v10), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov11/#directory-structure-of-yolov11-dataset","title":"Directory Structure of YOLOv11 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov11/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov11/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov11/#benefits-of-yolov11-format","title":"Benefits of YOLOv11 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov11/#example-of-yolov11-format","title":"Example of YOLOv11 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov11/#normalizing-bounding-box-coordinates-for-yolov11","title":"Normalizing Bounding Box Coordinates for YOLOv11","text":"<p>To convert pixel values to normalized values required by YOLOv11:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov11/#converting-annotations-to-yolov11-format-with-labelformat","title":"Converting Annotations to YOLOv11 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv11 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov11/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov11/#conversion-example-coco-to-yolov11","title":"Conversion Example: COCO to YOLOv11","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv11. Here's how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv11: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov11 \\\n    --output-folder dataset/yolov11_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov11_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov11/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov5/","title":"YOLOv5 Object Detection Format","text":""},{"location":"formats/object-detection/yolov5/#overview","title":"Overview","text":"<p>YOLOv5 is a well-known object detection model in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLO series, YOLOv5 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv5 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv5's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv5 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv5 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov5</p>"},{"location":"formats/object-detection/yolov5/#specification-of-yolov5-detection-format","title":"Specification of YOLOv5 Detection Format","text":"<p>The YOLOv5 detection format remains consistent with previous versions, ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov5/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov5/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov5/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov5/#benefits-of-yolov5-format","title":"Benefits of YOLOv5 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov5/#example-of-yolov5-format","title":"Example of YOLOv5 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov5/#normalizing-bounding-box-coordinates-for-yolov5","title":"Normalizing Bounding Box Coordinates for YOLOv5","text":"<p>To convert pixel values to normalized values required by YOLOv5:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov5/#converting-annotations-to-yolov5-format-with-labelformat","title":"Converting Annotations to YOLOv5 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv5 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov5/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov5/#conversion-example-coco-to-yolov5","title":"Conversion Example: COCO to YOLOv5","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv5. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv5: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov5 \\\n    --output-folder dataset/yolov5_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov5_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov5/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov6/","title":"YOLOv6 Object Detection Format","text":""},{"location":"formats/object-detection/yolov6/#overview","title":"Overview","text":"<p>YOLOv6 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv6: A Single-Stage Object Detection Framework for Industrial Applications. Building upon the foundations of YOLOv5, YOLOv6 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv6 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv6's enhanced performance.</p>"},{"location":"formats/object-detection/yolov6/#specification-of-yolov6-detection-format","title":"Specification of YOLOv6 Detection Format","text":"<p>The YOLOv6 detection format remains consistent with previous versions (such as YOLOv5), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov6/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov6/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov6/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov6/#benefits-of-yolov6-format","title":"Benefits of YOLOv6 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov6/#example-of-yolov6-format","title":"Example of YOLOv6 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov6/#normalizing-bounding-box-coordinates-for-yolov6","title":"Normalizing Bounding Box Coordinates for YOLOv6","text":"<p>To convert pixel values to normalized values required by YOLOv6:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov6/#converting-annotations-to-yolov6-format-with-labelformat","title":"Converting Annotations to YOLOv6 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv6 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov6/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov6/#conversion-example-coco-to-yolov6","title":"Conversion Example: COCO to YOLOv6","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv6. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv6: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov6 \\\n    --output-folder dataset/yolov6_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov6_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov6/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov7/","title":"YOLOv7 Object Detection Format","text":""},{"location":"formats/object-detection/yolov7/#overview","title":"Overview","text":"<p>YOLOv7 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors. Building upon the foundations of YOLOv5 through YOLOv6, YOLOv7 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv7 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv7's enhanced performance.</p>"},{"location":"formats/object-detection/yolov7/#specification-of-yolov7-detection-format","title":"Specification of YOLOv7 Detection Format","text":"<p>The YOLOv7 detection format remains consistent with previous versions (v5-v6), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov7/#directory-structure-requirements","title":"Directory Structure Requirements","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov7/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov7/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov7/#benefits-of-yolov7-format","title":"Benefits of YOLOv7 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov7/#example-of-yolov7-format","title":"Example of YOLOv7 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov7/#normalizing-bounding-box-coordinates-for-yolov7","title":"Normalizing Bounding Box Coordinates for YOLOv7","text":"<p>To convert pixel values to normalized values required by YOLOv7:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov7/#converting-annotations-to-yolov7-format-with-labelformat","title":"Converting Annotations to YOLOv7 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv7 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov7/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov7/#conversion-example-coco-to-yolov7","title":"Conversion Example: COCO to YOLOv7","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv7. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv7: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov7 \\\n    --output-folder dataset/yolov7_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov7_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov7/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov8/","title":"YOLOv8 Object Detection Format","text":""},{"location":"formats/object-detection/yolov8/#overview","title":"Overview","text":"<p>YOLOv8 is a well-known object detection model in the You Only Look Once (YOLO) series, renowned for its real-time object detection capabilities. Building upon the foundations of YOLOv5 through YOLOv7, YOLOv8 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv8 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv8's enhanced performance.</p> <p>Info: Unlike other YOLO versions, YOLOv8 was not introduced through an academic paper. The most commonly referenced implementation is the Ultralytics YOLOv8 repository on GitHub, which has become the de facto standard implementation.    For implementation details and specifications, see: GitHub Repository: ultralytics/yolov8</p>"},{"location":"formats/object-detection/yolov8/#specification-of-yolov8-detection-format","title":"Specification of YOLOv8 Detection Format","text":"<p>The YOLOv8 detection format remains consistent with previous versions (v5-v8), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov8/#directory-structure-of-yolov8-dataset","title":"Directory Structure of YOLOv8 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov8/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov8/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov8/#benefits-of-yolov8-format","title":"Benefits of YOLOv8 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov8/#example-of-yolov8-format","title":"Example of YOLOv8 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov8/#normalizing-bounding-box-coordinates-for-yolov8","title":"Normalizing Bounding Box Coordinates for YOLOv8","text":"<p>To convert pixel values to normalized values required by YOLOv8:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov8/#converting-annotations-to-yolov8-format-with-labelformat","title":"Converting Annotations to YOLOv8 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv8 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov8/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov8/#conversion-example-coco-to-yolov8","title":"Conversion Example: COCO to YOLOv8","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv8. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv8: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov8 \\\n    --output-folder dataset/yolov8_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov8_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov8/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"formats/object-detection/yolov9/","title":"YOLOv9 Object Detection Format","text":""},{"location":"formats/object-detection/yolov9/#overview","title":"Overview","text":"<p>YOLOv9 is a well-known object detection model in the You Only Look Once (YOLO) series, introduced in the paper YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information. Building upon the foundations of YOLOv5 through YOLOv8, YOLOv9 introduces significant advancements in model architecture and training methodologies, enhancing both accuracy and efficiency. Despite these improvements, YOLOv9 retains the same object detection format as its predecessors, utilizing normalized coordinates in text files. This consistency ensures seamless integration and compatibility with existing workflows while benefiting from YOLOv9's enhanced performance.</p>"},{"location":"formats/object-detection/yolov9/#specification-of-yolov9-detection-format","title":"Specification of YOLOv9 Detection Format","text":"<p>The YOLOv9 detection format remains consistent with previous versions (v5-v8), ensuring ease of adoption and compatibility. Below are the detailed specifications:</p> <ul> <li> <p>One Text File per Image:   For every image in your dataset, there exists a corresponding <code>.txt</code> file containing annotation data.</p> </li> <li> <p>Object Representation:   Each line in the text file represents a single object detected within the image, following the format: <code>&lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;</code> </p> <ul> <li><code>&lt;class_id&gt;</code> (Integer):   An integer representing the object's class.</li> <li><code>&lt;x_center&gt;</code> and <code>&lt;y_center&gt;</code> (Float): The normalized coordinates of the object's center relative to the image's width and    height.</li> <li><code>&lt;width&gt;</code> and <code>&lt;height&gt;</code> (Float): The normalized width and height of the bounding box encompassing the object.</li> </ul> </li> <li> <p>Normalization of Values:   All coordinate and size values are normalized to a range between <code>0.0</code> and <code>1.0</code>.  </p> </li> <li> <p>Normalization Formula:     To convert pixel values to normalized coordinates: <pre><code>normalized_x = x_pixel / image_width  \nnormalized_y = y_pixel / image_height  \nnormalized_width = box_width_pixel / image_width  \nnormalized_height = box_height_pixel / image_height\n</code></pre></p> </li> <li> <p>Class ID Indexing:   Class IDs start from <code>0</code>, with each ID corresponding to a specific object category defined in the <code>data.yaml</code> configuration file. Class IDs must be contiguous integers (e.g., 0,1,2 and not 0,2,3) to ensure proper model training and inference.</p> </li> <li> <p>Configuration via <code>data.yaml</code>:   The <code>data.yaml</code> file contains essential configuration settings, including paths to training and validation datasets, number of classes (<code>nc</code>), and a mapping of class names to their respective IDs (<code>names</code>).</p> </li> </ul>"},{"location":"formats/object-detection/yolov9/#directory-structure-of-yolov9-dataset","title":"Directory Structure of YOLOv9 Dataset","text":"<p>The dataset must maintain a parallel directory structure for images and their corresponding label files. There are two common organizational patterns:</p>"},{"location":"formats/object-detection/yolov9/#pattern-1-images-and-labels-as-root-directories","title":"Pattern 1: Images and Labels as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 train/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 val/\n\u2502       \u251c\u2500\u2500 image3.jpg\n\u2502       \u2514\u2500\u2500 image4.jpg\n\u2514\u2500\u2500 labels/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 image1.txt\n    \u2502   \u2514\u2500\u2500 image2.txt\n    \u2514\u2500\u2500 val/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre>"},{"location":"formats/object-detection/yolov9/#pattern-2-trainval-as-root-directories","title":"Pattern 2: Train/Val as Root Directories","text":"<pre><code>dataset/\n\u251c\u2500\u2500 data.yaml\n\u251c\u2500\u2500 train/\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u2514\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n\u2514\u2500\u2500 val/\n    \u251c\u2500\u2500 images/\n    \u2502   \u251c\u2500\u2500 image3.jpg\n    \u2502   \u2514\u2500\u2500 image4.jpg\n    \u2514\u2500\u2500 labels/\n        \u251c\u2500\u2500 image3.txt\n        \u2514\u2500\u2500 image4.txt\n</code></pre> <p>The corresponding <code>data.yaml</code> configuration should match your chosen structure:</p> <pre><code># For Pattern 1:\npath: .  # Optional - defaults to current directory if omitted\ntrain: images/train  # Path to training images\nval: images/val      # Path to validation images\n\n# For Pattern 2:\npath: .  # Optional - defaults to current directory if omitted\ntrain: train/images  # Path to training images\nval: val/images      # Path to validation images\n</code></pre> <p>Important: Label files must have the same name as their corresponding image files (excluding the file extension) and must maintain the parallel directory structure, only replacing <code>images</code> with <code>labels</code> in the path. </p>"},{"location":"formats/object-detection/yolov9/#benefits-of-yolov9-format","title":"Benefits of YOLOv9 Format","text":"<ul> <li>Simplicity: Easy to read and write, facilitating quick dataset preparation.</li> <li>Efficiency: Compact representation reduces storage requirements.</li> <li>Compatibility: Maintains consistency across YOLO versions, ensuring seamless integration with various tools and frameworks.</li> </ul>"},{"location":"formats/object-detection/yolov9/#example-of-yolov9-format","title":"Example of YOLOv9 Format","text":"<p>Example of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory (defaults to current directory if omitted). Can also be `../dataset`.\ntrain: images/train  # Directory for training images\nval: images/val  # Directory for validation images\ntest: images/test  # Directory for test images (optional)\n\nnames:\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Example Annotation</p> <p>For an image named <code>image1.jpg</code>, the corresponding <code>image1.txt</code> might contain: <pre><code>0 0.716797 0.395833 0.216406 0.147222\n1 0.687500 0.379167 0.255208 0.175000\n</code></pre></p> <p>Explanation: - The first line represents an object of class <code>0</code> (e.g., <code>cat</code>) with its bounding box centered at <code>(0.716797, 0.395833)</code> relative to the image dimensions, and a width and height of <code>0.216406</code> and <code>0.147222</code> respectively. - The second line represents an object of class <code>1</code> (e.g., <code>dog</code>) with its own bounding box specifications.</p>"},{"location":"formats/object-detection/yolov9/#normalizing-bounding-box-coordinates-for-yolov9","title":"Normalizing Bounding Box Coordinates for YOLOv9","text":"<p>To convert pixel values to normalized values required by YOLOv9:</p> <pre><code># Given pixel values and image dimensions\nx_top_left = 150     # x coordinate of top-left corner of bounding box\ny_top_left = 200     # y coordinate of top-left corner of bounding box\nwidth_pixel = 50     # width of bounding box\nheight_pixel = 80    # height of bounding box\nimage_width = 640\nimage_height = 480\n\n# 1. Convert top-left coordinates to center coordinates (in pixels)\nx_center_pixel = x_top_left + (width_pixel / 2)   # 150 + (50/2) = 175\ny_center_pixel = y_top_left + (height_pixel / 2)  # 200 + (80/2) = 240\n\n# 2. Normalize all values (divide by image dimensions)\nx_center = x_center_pixel / image_width     # 175 / 640 = 0.273438\ny_center = y_center_pixel / image_height    # 240 / 480 = 0.500000\nwidth = width_pixel / image_width           # 50 / 640 = 0.078125\nheight = height_pixel / image_height        # 80 / 480 = 0.166667\n\n# Annotation line format: &lt;class_id&gt; &lt;x_center&gt; &lt;y_center&gt; &lt;width&gt; &lt;height&gt;\nannotation = f\"0 {x_center} {y_center} {width} {height}\"\n# Output: \"0 0.273438 0.500000 0.078125 0.166667\"\n</code></pre>"},{"location":"formats/object-detection/yolov9/#converting-annotations-to-yolov9-format-with-labelformat","title":"Converting Annotations to YOLOv9 Format with Labelformat","text":"<p>Our Labelformat framework simplifies the process of converting various annotation formats to the YOLOv9 detection format. Below is a step-by-step guide to perform this conversion.</p>"},{"location":"formats/object-detection/yolov9/#installation","title":"Installation","text":"<p>First, ensure that Labelformat is installed. You can install it via pip:</p> <pre><code>pip install labelformat\n</code></pre>"},{"location":"formats/object-detection/yolov9/#conversion-example-coco-to-yolov9","title":"Conversion Example: COCO to YOLOv9","text":"<p>Assume you have annotations in the COCO format and wish to convert them to YOLOv9. Here\u2019s how you can achieve this using Labelformat.</p> <p>Step 1: Prepare Your Dataset</p> <p>Ensure your dataset follows the standard COCO structure:</p> <ul> <li>You have a <code>.json</code> file with the COCO annotations. (e.g. <code>annotations/instances_train.json</code>)</li> <li>You have a directory with the images. (e.g. <code>images/</code>)</li> </ul> <p>Full example: <pre><code>dataset/\n\u251c\u2500\u2500 annotations/\n\u2502   \u2514\u2500\u2500 instances_train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2514\u2500\u2500 ...\n</code></pre></p> <p>Step 2: Run the Conversion Command</p> <p>Use the Labelformat CLI to convert COCO annotations to YOLOv9: <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file dataset/annotations/instances_train.json \\\n    --output-format yolov9 \\\n    --output-folder dataset/yolov9_labels \\\n    --output-split train\n</code></pre></p> <p>Step 3: Verify the Converted Annotations</p> <p>After conversion, your dataset structure will be: <pre><code>dataset/\n\u251c\u2500\u2500 yolov9_labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u251c\u2500\u2500 images/\n\u2502   \u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2502   \u251c\u2500\u2500 image2.jpg\n\u2502   \u2502   \u2514\u2500\u2500 ...\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u251c\u2500\u2500 image2.txt\n\u2502       \u2514\u2500\u2500 ...\n</code></pre></p> <p>Contents of <code>data.yaml</code>: <pre><code>path: .  # Dataset root directory\ntrain: images  # Directory for training images\nnc: 3  # Number of classes\nnames:  # Class name mapping\n  0: cat\n  1: dog\n  2: person\n</code></pre></p> <p>Contents of <code>image1.txt</code>: <pre><code>0 0.234375 0.416667 0.078125 0.166667\n1 0.500000 0.500000 0.100000 0.200000\n</code></pre></p>"},{"location":"formats/object-detection/yolov9/#error-handling-in-labelformat","title":"Error Handling in Labelformat","text":"<p>The format implementation includes several safeguards:</p> <ul> <li>Missing Label Files:</li> <li>Warning is logged if a label file doesn't exist for an image</li> <li> <p>Image is skipped from processing</p> </li> <li> <p>File Access Issues:</p> </li> <li>Errors are logged if label files cannot be read due to permissions or other OS issues</li> <li> <p>Affected images are skipped from processing</p> </li> <li> <p>Label Format Validation:</p> </li> <li>Each line must contain exactly 5 space-separated values</li> <li>Invalid lines are logged as warnings and skipped</li> <li>All values must be convertible to appropriate types:<ul> <li>Category ID must be a valid integer</li> <li>Coordinates and dimensions must be valid floats</li> </ul> </li> <li>Category IDs must exist in the category mapping</li> </ul> <p>Example of a properly formatted label file: <pre><code>0 0.716797 0.395833 0.216406 0.147222  # Each line must have exactly 5 space-separated values\n1 0.687500 0.379167 0.255208 0.175000  # All values must be valid numbers within [0,1] range\n</code></pre></p>"},{"location":"tutorials/converting-coco-to-yolov8/","title":"Converting COCO Labels to YOLOv8 Format","text":"<p>This tutorial walks you through converting object detection labels from the COCO format to the YOLOv8 format using Labelformat's CLI and Python API.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#prerequisites","title":"Prerequisites","text":"<ul> <li>Labelformat Installed: Follow the Installation Guide.</li> <li>COCO Dataset: Ensure you have a COCO-formatted dataset ready for conversion.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-1-prepare-your-dataset","title":"Step 1: Prepare Your Dataset","text":"<p>Organize your dataset with the following structure:</p> <pre><code>project/\n\u251c\u2500\u2500 coco-labels/\n\u2502   \u2514\u2500\u2500 train.json\n\u251c\u2500\u2500 images/\n\u2502   \u251c\u2500\u2500 image1.jpg\n\u2502   \u2514\u2500\u2500 image2.jpg\n</code></pre> <p>Ensure that <code>train.json</code> contains the COCO annotations and that all images are located in the <code>images/</code> directory.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#step-2-using-the-cli-for-conversion","title":"Step 2: Using the CLI for Conversion","text":"<p>Open your terminal and navigate to the <code>project/</code> directory.</p> <p>Run the following command to convert COCO labels to YOLOv8:</p> <pre><code>labelformat convert \\\n    --task object-detection \\\n    --input-format coco \\\n    --input-file coco-labels/train.json \\\n    --output-format yolov8 \\\n    --output-file yolo-labels/data.yaml \\\n    --output-split train\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#explanation-of-the-command","title":"Explanation of the Command:","text":"<ul> <li><code>--task object-detection</code>: Specifies the task type.</li> <li><code>--input-format coco</code>: Defines the input label format.</li> <li><code>--input-file coco-labels/train.json</code>: Path to the COCO annotations file.</li> <li><code>--output-format yolov8</code>: Desired output format.</li> <li><code>--output-file yolo-labels/data.yaml</code>: Path to save the YOLOv8 configuration file.</li> <li><code>--output-split train</code>: Data split label.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-3-verify-the-conversion","title":"Step 3: Verify the Conversion","text":"<p>After running the command, your project structure should include:</p> <pre><code>project/\n\u251c\u2500\u2500 yolo-labels/\n\u2502   \u251c\u2500\u2500 data.yaml\n\u2502   \u2514\u2500\u2500 labels/\n\u2502       \u251c\u2500\u2500 image1.txt\n\u2502       \u2514\u2500\u2500 image2.txt\n</code></pre> <ul> <li><code>data.yaml</code>: YOLOv8 configuration file containing category names and paths.</li> <li><code>labels/</code>: Directory containing YOLOv8-formatted label files.</li> </ul> <p>Sample <code>data.yaml</code>: <pre><code>names:\n  0: cat\n  1: dog\n  2: fish\nnc: 3\npath: .\ntrain: images\n</code></pre></p> <p>Sample Label File (<code>image1.txt</code>):</p> <pre><code>2 0.8617 0.7308 0.0359 0.0433\n0 0.8180 0.6911 0.0328 0.0793\n</code></pre> <ul> <li>Format: <code>&lt;category_id&gt; &lt;center_x&gt; &lt;center_y&gt; &lt;width&gt; &lt;height&gt;</code></li> <li>Coordinates: Normalized between 0 and 1.</li> </ul>"},{"location":"tutorials/converting-coco-to-yolov8/#step-4-using-the-python-api-for-conversion","title":"Step 4: Using the Python API for Conversion","text":"<p>If you prefer using Python for more control, follow these steps.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#41-write-the-conversion-script","title":"4.1: Write the Conversion Script","text":"<p>Create a Python script named <code>coco_to_yolov8.py</code> with the following content:</p> <pre><code>from pathlib import Path\nfrom labelformat.formats import COCOObjectDetectionInput, YOLOv8ObjectDetectionOutput\n\n# Define input and output paths\ncoco_input_path = Path(\"coco-labels/train.json\")\nyolo_output_path = Path(\"yolo-labels/data.yaml\")\n\n# Initialize input and output classes\ncoco_input = COCOObjectDetectionInput(input_file=coco_input_path)\nyolo_output = YOLOv8ObjectDetectionOutput(\n    output_file=yolo_output_path,\n    output_split=\"train\"\n)\n\n# Perform the conversion\nyolo_output.save(label_input=coco_input)\n\nprint(\"Conversion from COCO to YOLOv8 completed successfully!\")\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#42-execute-the-script","title":"4.2: Execute the Script","text":"<p>Run the script using Python:</p> <pre><code>python coco_to_yolov8.py\n</code></pre> <p>Upon successful execution, you will see:</p> <pre><code>Conversion from COCO to YOLOv8 completed successfully!\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#step-5-integrate-with-your-training-pipeline","title":"Step 5: Integrate with Your Training Pipeline","text":"<p>Use the generated YOLOv8 labels (<code>data.yaml</code> and <code>labels/</code> directory) to train your YOLOv8 models seamlessly.</p> <p>Example YOLOv8 Training Command:</p> <pre><code>yolo detect train data=yolo-labels/data.yaml model=yolov8s.pt epochs=100 imgsz=640\n</code></pre>"},{"location":"tutorials/converting-coco-to-yolov8/#conclusion","title":"Conclusion","text":"<p>You've successfully converted COCO labels to YOLOv8 format using both the CLI and Python API. Labelformat simplifies label format conversions, enabling efficient integration into your computer vision projects.</p>"},{"location":"tutorials/converting-coco-to-yolov8/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Converting YOLOv8 to COCO.</li> <li>Learn how to Handle Labelbox Exports.</li> <li>Dive deeper with Advanced Usage.</li> </ul> <p>For any questions or issues, feel free to reach out via our GitHub Issues.</p>"}]}